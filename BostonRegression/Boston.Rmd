---
title: "Boston"
author: "Laura del Pino Díaz"
date: "24/11/2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(MASS)
```

## Boston

Boston es un conjunto de datos en el que están almacenados los distintos parámetros de las casas de la ciudad de Boston. El objetivo es intentar predecir el valor de las casas sabiendo el valor de un conjunto de ellas. 

#Los parámetros

* crim - proporción de crimen per cápita en el barrio.
* zn - proporción de zona residencial por cada 25 000 pies cuadrados
* indus - proporción de acres de negocios industriales.
* chas - indica si la casa da al río Charles o no.
* nox - concentración de óxido de nitrógenso en partes por 10 millones.
* rm - número de habitaciones
* age - proporción de viviendas anteriores a 1940.
* dis - media ponderada de distancias a los cinco centros de empleo de Boston.
* rad - índice de accesibilidad a los pasos altos.
* tax - impuestos
* ptratio - proporción de profesores.
* black - 100(Bk -0.63)^2 donde Bk es la proporción de black en el pueblo
* lstat - porcentaje de la población de estatus bajo
*  medv - valor de la casa en 1000 dólares 

  Se aconseja ejecutar el comando attach para acceder directamente a los campos.

```{r Boston}
attach(Boston)
head(lstat)

```

## Visualización de datos

 Para comprobar que datos intervienen en el valor final es necesario visualizar los datos con respecto de la salida. 
  Esto lo podemos conseguir mirando cada una de las variables con respecto de la salida

```{r , echo=FALSE}
plot(Boston$medv~age,Boston)
```

   O directamente mirando todas las variables entre sí o con respecto a la salida.

```{r,echo=FALSE}
temp <-Boston
plotY<-function (x,y) {
plot(temp[,y]~temp[,x], xlab=paste(names(temp)[x]," X",x,sep=""), ylab=names(temp)[y])
}
par(mfrow=c(3,4))
x <-sapply(1:(dim(temp)[2]-1), plotY, dim(temp)[2])
par(mfrow=c(1,1))
```
\newpage 

  Y una vez las comparamos todas elegimos las que nos parecen más relevantes. 
   
   
```{r,echo=FALSE}
   par(mfrow=c(3,3))
x <-sapply(c(1, 5, 6, 7, 8, 10, 11, 12, 13), plotY, dim(temp)[2])
par(mfrow=c(1,1))
```

    En este caso nos parece que tienen un ajustes lineal las variables rm y lstat.
    
## Obtención de un modelo lineal

    Para obtener el modelo lineal sobre una variable utilizamos la función lm, que tiene como parámetros la fórmula y el conjunto de datos. 

```{r}
fit1 = lm(medv ~lstat, data = Boston)
fit2 = lm(medv~rm,data=Boston)
```

    Ahora que tenemos el modelo lineal, vamos a comprobar la bondad del mismo para ello ejecutamos un summary para que nos de los datos

```{r}
summary(fit1)
summary(fit2)
```

    Para saber cuan bueno es el modelo lineal nos fijaremos en el parámetro de salida de la llamada al summary R-squared o R cuadrada. 
    Este parámetro cuanto más cercano a 1 mejor, por lo tanto en estos modelos que hemos generado donde R-squared es de 0.54 y 0.48 podemos decir que no son muy buenos.
    
### Accediendo a la información del modelo
    
    Para saber que información tiene nuestro modelo podemos consultar los campor realizando una llamada a names que nos devolverá todos los nombres de las variables almacenadas en el modelo
    
```{r}
names(fit1)
```
    
    Para calcular la raíz del error cuadrático mínimo utilizamos el siguiente  comando
    
    ```{r}
    sqrt(sum(fit1$residuals^2)/length(fit1$residuals))
    ```
    Este error cuadrático mínimo no coincide con el que nos devolvía y eso es porque se utiliza el n-p para calcularlo. 
    ```{r}
    sqrt(sum(fit1$residuals^2)/(length(fit1$residuals)-2))
    ```
    En este caso al ajustar el denominador con -2  si se obtiene el coeficiente que nos devolvió el summary. 
    
##Predicción sobre nuevos datos.

  El fin último de la regresión es la estimación del valor del parámetro de salida a partir del modelo. Para predecir con el modelo que hemos creado anteriormente utilizaremos el siguiente comando:

```{r} 
  predict(fit1, data.frame(lstat=c(5,10,15)))
```

  El cálculo de la raíz del error cuadrático mínimo para el conjunto de test lo hacemos ejecutando la siguiente sentencia:
  
```{r}
   yprime=predict(fit1,data.frame(lstat=Boston$lstat))
   sqrt(sum(abs(Boston$medv-yprime)^2)/length(yprime))
```
  
#Modelo de regresión lineal múltiple
   
    Dado que el modelo lineal con una sola variable no es lo suficientemente bueno podemos añadirles variables a nuestro modelo 
    
```{r}
  fit3=lm(medv~lstat+age,data=Boston)
  summary(fit3)
```

   En este primer modelo lineal compuesto vemos tres variables, la combinación de lstat y age, así como cada una de las variables por separado. 
   Ante el test de hipótesis de vemos que la variable age está marcada por dos **, esto no es buena señal puesto que indica que aporta menos que las demás a la salida en el modelo. 
   
   Sabiendo esto cambiamos la variable age por rm y comprobamos que es lo que obtenemos

```{r}
fit4=lm(medv~lstat+rm,data=Boston)
summary(fit4)
```

  Ahora la variable de nuestro modelo que no interviene es la combinación de ambas, por lo que seguiremos modificando el modelo pero esta vez no lo haremos al tuntún sino que probaremos con el conjunto de todas las variables.
  
```{r}
fit5=lm(medv~.,data=Boston)
summary(fit5)
```
 
  Todas aquellas variables con un alto Pr son candidatas a ser eliminadas. Por ello vamos a eliminar de nuestro modelo a las variables age e indus
  
```{r}
fit6=lm(medv~.-age-indus,data=Boston)
summary(fit6)
```

   Comparando las dos R cuadradas parece que hemos mejorado levemente el modelo.  Probemos a eliminar aquellas variables marcadas únuicamente con ** 
   
```{r}
fit7=lm(medv~.-age-indus-chas-crim,data=Boston)
summary(fit7)
```
  
  En este caso vemos que empeora con respecto al caso anterior en casi un 1%
  
##Iteracciones y no linealidad

#Interacción
 
   Probemos esta vez con interrelaciones entre variables.
   
```{r}
fit8=lm(medv~lstat*rm,Boston)
summary(fit8)
```

  Esta combinación nos da el mejor modelo hasta la fecha. 
  
#No linealidad
   
  Si asumimos que el modelo no es lineal porque la intervención de alguna variable no lo sea, lo podemos probar haciendo uso de la función l

```{r}
fit9=lm(medv~I(lstat^2),Boston)
summary(fit9)
```
  
  Lo que hay que tener en cuenta que hay que poner un término por cada grado hasta llegar al grado que deseamos. 

```{r}
  fit9=lm(medv~lstat+I(lstat^2),Boston)
summary(fit9)
```

  En el caso de que el grado sea muy grande podemos hacer uso de la función poly para que nos lo haga R.

```{r}
  fit10=lm(medv~poly(lstat,18))
summary(fit10)
```

   En este resultado podemos ver como a partir del grado 5, las variables de grado superior dejan de ser importantes para el modelo. 
   